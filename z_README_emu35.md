# Emu3.5 æ¨¡å‹ç»“æ„è¯¦ç»†åˆ†æ

## ğŸ“‘ ç›®å½•å¯¼èˆª

- [1. æ ¸å¿ƒæ¶æ„æ¦‚è¿°](#1-æ ¸å¿ƒæ¶æ„æ¦‚è¿°)
- [2. æ¨¡å‹æ¶æ„å±‚æ¬¡ç»“æ„](#2-æ¨¡å‹æ¶æ„å±‚æ¬¡ç»“æ„)
- [3. è§†è§‰ç¼–ç å™¨](#3-è§†è§‰ç¼–ç å™¨vision-tokenizer-ibq)
- [4. æ–‡æœ¬tokenizerä¸ç‰¹æ®Štoken](#4-æ–‡æœ¬tokenizerä¸ç‰¹æ®Štoken)
- [5. ç”Ÿæˆè¿‡ç¨‹ä¸æ¨ç†](#5-ç”Ÿæˆè¿‡ç¨‹ä¸æ¨ç†)
- [6. ä»»åŠ¡ç±»å‹ä¸æ¨¡æ¿](#6-ä»»åŠ¡ç±»å‹ä¸æ¨¡æ¿)
- [7. å…³é”®åˆ›æ–°ç‚¹](#7-å…³é”®åˆ›æ–°ç‚¹)
- [8. æ¨¡å‹é…ç½®å‚æ•°æ€»ç»“](#8-æ¨¡å‹é…ç½®å‚æ•°æ€»ç»“)
- [9. æ¨ç†æ¡†æ¶æ”¯æŒ](#9-æ¨ç†æ¡†æ¶æ”¯æŒ)
- [10. è¾“å‡ºæ ¼å¼](#10-è¾“å‡ºæ ¼å¼)
- [11. è¯¦ç»†çš„å‰å‘ä¼ æ’­æµç¨‹](#11-è¯¦ç»†çš„å‰å‘ä¼ æ’­æµç¨‹)
- [12. æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£](#12-æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£)
- [13. ç”Ÿæˆç­–ç•¥è¯¦è§£](#13-ç”Ÿæˆç­–ç•¥è¯¦è§£)
- [14. å†…å­˜ä¼˜åŒ–æŠ€æœ¯](#14-å†…å­˜ä¼˜åŒ–æŠ€æœ¯)
- [15. æ¨¡å‹å˜ä½“](#15-æ¨¡å‹å˜ä½“)
- [16. ä»£ç å®ç°å…³é”®ç‚¹](#16-ä»£ç å®ç°å…³é”®ç‚¹)
- [17. æ€§èƒ½æŒ‡æ ‡](#17-æ€§èƒ½æŒ‡æ ‡)
- [18. å¸¸è§é—®é¢˜ä¸ä¼˜åŒ–å»ºè®®](#18-å¸¸è§é—®é¢˜ä¸ä¼˜åŒ–å»ºè®®)
- [19. ä¸å…¶ä»–æ¨¡å‹çš„å¯¹æ¯”](#19-ä¸å…¶ä»–æ¨¡å‹çš„å¯¹æ¯”)
- [20. æ‰©å±•ä¸æ”¹è¿›æ–¹å‘](#20-æ‰©å±•ä¸æ”¹è¿›æ–¹å‘)

---

## ğŸš€ å¿«é€Ÿå‚è€ƒ

### æ¨¡å‹è§„æ ¼
- **å‚æ•°é‡**ï¼š8.2B (82äº¿)
- **éšè—ç»´åº¦**ï¼š4,096
- **å±‚æ•°**ï¼š32
- **æ³¨æ„åŠ›å¤´**ï¼š32 (GQA: 8ä¸ªKVå¤´)
- **è¯æ±‡è¡¨**ï¼š184,622
- **æœ€å¤§åºåˆ—**ï¼š9,216 tokens
- **å›¾åƒåˆ†è¾¨ç‡**ï¼š720Ã—720

### å…³é”®ç‰¹æ€§
| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| æ¶æ„ | ç»Ÿä¸€Transformer (æ— é€‚é…å™¨) |
| ä½ç½®ç¼–ç  | RoPE (æ”¯æŒç¼©æ”¾) |
| æ³¨æ„åŠ› | GQA + Flash Attention 2 |
| é‡åŒ– | IBQ (ç´¢å¼•ä¼ æ’­é‡åŒ–) |
| åŠ é€Ÿ | DiDA (20å€åŠ é€Ÿ) |
| æ¨ç† | Transformers + vLLM |

### å¿«é€Ÿå¼€å§‹
```python
from src.utils.model_utils import build_emu3p5

# åŠ è½½æ¨¡å‹
model, tokenizer, vq_model = build_emu3p5(
    model_path="BAAI/Emu3.5-Image",
    tokenizer_path="./src/tokenizer_emu3_ibq",
    vq_path="BAAI/Emu3.5-VisionTokenizer",
    vq_device="cuda:0"
)

# ç”Ÿæˆå›¾åƒ
from src.utils.generation_utils import generate
outputs = generate(cfg, model, tokenizer, input_ids, unconditional_ids)
```

---

## 1. æ ¸å¿ƒæ¶æ„æ¦‚è¿°

Emu3.5æ˜¯ä¸€ä¸ª**åŸç”Ÿå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹**ï¼Œé‡‡ç”¨**ç»Ÿä¸€çš„ä¸‹ä¸€ä¸ªtokené¢„æµ‹**ç›®æ ‡ï¼Œåœ¨**äº¤é”™çš„è§†è§‰-è¯­è¨€åºåˆ—**ä¸Šè¿›è¡Œç«¯åˆ°ç«¯é¢„è®­ç»ƒã€‚æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

- **ç»Ÿä¸€ä¸–ç•Œå»ºæ¨¡**ï¼šè”åˆé¢„æµ‹è§†è§‰å’Œè¯­è¨€çš„ä¸‹ä¸€ä¸ªçŠ¶æ€
- **åŸç”Ÿå¤šæ¨¡æ€I/O**ï¼šæ— éœ€æ¨¡æ€é€‚é…å™¨ï¼Œç›´æ¥å¤„ç†å’Œç”Ÿæˆäº¤é”™çš„è§†è§‰-æ–‡æœ¬åºåˆ—
- **10T+å¤šæ¨¡æ€tokené¢„è®­ç»ƒ**ï¼šåœ¨è§†é¢‘å¸§å’Œè½¬å½•æ–‡æœ¬ä¸Šè¿›è¡Œå¤§è§„æ¨¡é¢„è®­ç»ƒ
- **ç¦»æ•£æ‰©æ•£é€‚é…(DiDA)**ï¼šå°†é¡ºåºè§£ç è½¬æ¢ä¸ºåŒå‘å¹¶è¡Œé¢„æµ‹ï¼Œå®ç°â‰ˆ20å€æ¨ç†åŠ é€Ÿ

### 1.1 æ•´ä½“æ¶æ„æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        è¾“å…¥å¤„ç†å±‚                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ–‡æœ¬æç¤º â”€â”€â†’ Tokenizer â”€â”€â†’ Token IDs                            â”‚
â”‚  å‚è€ƒå›¾åƒ â”€â”€â†’ IBQç¼–ç å™¨ â”€â”€â†’ è§†è§‰Token IDs                        â”‚
â”‚  â†“                                                               â”‚
â”‚  åˆå¹¶äº¤é”™åºåˆ— â”€â”€â†’ [BOS, text_token, visual_token, ..., EOS]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Embedding & Normalization                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Token IDs â”€â”€â†’ Embedding Layer â”€â”€â†’ [batch, seq_len, 4096]      â”‚
â”‚                                    â†“                             â”‚
â”‚                              Dropout (0.1)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              32å±‚ Transformer Decoder Layers                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Layer i (é‡å¤32æ¬¡)                                       â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ 1. Input LayerNorm (RMSNorm)                            â”‚   â”‚
â”‚  â”‚    â†“                                                     â”‚   â”‚
â”‚  â”‚ 2. Self-Attention (GQA)                                 â”‚   â”‚
â”‚  â”‚    â”œâ”€ Q,K,VæŠ•å½±                                         â”‚   â”‚
â”‚  â”‚    â”œâ”€ RoPEä½ç½®ç¼–ç                                       â”‚   â”‚
â”‚  â”‚    â”œâ”€ æ³¨æ„åŠ›è®¡ç®—                                        â”‚   â”‚
â”‚  â”‚    â””â”€ è¾“å‡ºæŠ•å½±                                          â”‚   â”‚
â”‚  â”‚    â†“                                                     â”‚   â”‚
â”‚  â”‚ 3. Residual + Dropout                                   â”‚   â”‚
â”‚  â”‚    â†“                                                     â”‚   â”‚
â”‚  â”‚ 4. Post-Attention LayerNorm (RMSNorm)                   â”‚   â”‚
â”‚  â”‚    â†“                                                     â”‚   â”‚
â”‚  â”‚ 5. MLP (GLU)                                            â”‚   â”‚
â”‚  â”‚    â”œâ”€ Gate Projection                                   â”‚   â”‚
â”‚  â”‚    â”œâ”€ Up Projection                                     â”‚   â”‚
â”‚  â”‚    â”œâ”€ SiLU Activation                                   â”‚   â”‚
â”‚  â”‚    â””â”€ Down Projection                                   â”‚   â”‚
â”‚  â”‚    â†“                                                     â”‚   â”‚
â”‚  â”‚ 6. Residual + Dropout                                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â†“                                     â”‚
â”‚                    [batch, seq_len, 4096]                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Output Processing                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Final LayerNorm (RMSNorm)                                       â”‚
â”‚    â†“                                                             â”‚
â”‚  LM Head (Linear: 4096 â†’ 184622)                                â”‚
â”‚    â†“                                                             â”‚
â”‚  Logits [batch, seq_len, 184622]                                â”‚
â”‚    â†“                                                             â”‚
â”‚  Sampling/Decoding                                              â”‚
â”‚    â†“                                                             â”‚
â”‚  Next Token ID                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ç”Ÿæˆä¸è§£ç                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ–‡æœ¬Token â”€â”€â†’ Tokenizerè§£ç  â”€â”€â†’ æ–‡æœ¬                           â”‚
â”‚  è§†è§‰Token â”€â”€â†’ IBQè§£ç å™¨ â”€â”€â†’ å›¾åƒ                               â”‚
â”‚  â†“                                                               â”‚
â”‚  äº¤é”™è¾“å‡º (æ–‡æœ¬+å›¾åƒåºåˆ—)                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 å…³é”®åˆ›æ–°ç‚¹

1. **ç»Ÿä¸€Tokenç©ºé—´**
   - æ–‡æœ¬å’Œè§†è§‰å…±äº«åŒä¸€è¯æ±‡è¡¨
   - æ— éœ€ç‰¹æ®Šçš„æ¨¡æ€é€‚é…å™¨
   - æ”¯æŒä»»æ„é¡ºåºçš„äº¤é”™åºåˆ—

2. **é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶**
   - åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›(GQA)ï¼š32ä¸ªæŸ¥è¯¢å¤´ + 8ä¸ªKVå¤´
   - æ—‹è½¬ä½ç½®ç¼–ç (RoPE)ï¼šæ”¯æŒé•¿åºåˆ—
   - Flash Attention 2ï¼šGPUä¼˜åŒ–å®ç°

3. **è§†è§‰ç¼–ç **
   - IBQ (Index-Based Quantization)
   - 16å€ç©ºé—´å‹ç¼©
   - ç¦»æ•£tokenè¡¨ç¤º

4. **ç”Ÿæˆä¼˜åŒ–**
   - åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼(CFG)
   - å·®åˆ†é‡‡æ ·(ä¸åŒtokenç±»å‹ç”¨ä¸åŒå‚æ•°)
   - KVç¼“å­˜åŠ é€Ÿ

---

## 2. æ¨¡å‹æ¶æ„å±‚æ¬¡ç»“æ„

### 2.1 é¡¶å±‚æ¶æ„ï¼šEmu3ForCausalLM

```
Emu3ForCausalLM (å› æœè¯­è¨€æ¨¡å‹)
â”œâ”€â”€ Emu3Model (æ ¸å¿ƒTransformerè§£ç å™¨)
â”‚   â”œâ”€â”€ embed_tokens (è¯åµŒå…¥å±‚)
â”‚   â”œâ”€â”€ layers (32å±‚Transformerè§£ç å™¨å±‚)
â”‚   â”œâ”€â”€ norm (æœ€ç»ˆRMSNorm)
â”‚   â””â”€â”€ dropout
â””â”€â”€ lm_head (çº¿æ€§æŠ•å½±åˆ°è¯æ±‡è¡¨)
```

**å…³é”®å‚æ•°**ï¼ˆé»˜è®¤é…ç½®ï¼‰ï¼š
- vocab_size: 184,622
- hidden_size: 4,096
- num_hidden_layers: 32
- max_position_embeddings: 9,216
- image_area: 720Ã—720 (518,400åƒç´ )

### 2.2 Transformerè§£ç å™¨å±‚ï¼šEmu3DecoderLayer

æ¯ä¸ªè§£ç å™¨å±‚åŒ…å«ï¼š

```
Emu3DecoderLayer
â”œâ”€â”€ input_layernorm (RMSNorm)
â”œâ”€â”€ self_attn (è‡ªæ³¨æ„åŠ›æœºåˆ¶)
â”‚   â”œâ”€â”€ q_proj, k_proj, v_proj (æŠ•å½±å±‚)
â”‚   â”œâ”€â”€ q_norm, k_norm (æŸ¥è¯¢/é”®å½’ä¸€åŒ–)
â”‚   â”œâ”€â”€ rotary_emb (æ—‹è½¬ä½ç½®ç¼–ç )
â”‚   â””â”€â”€ o_proj (è¾“å‡ºæŠ•å½±)
â”œâ”€â”€ post_attention_layernorm (RMSNorm)
â”œâ”€â”€ mlp (å‰é¦ˆç½‘ç»œ)
â”‚   â”œâ”€â”€ gate_proj (é—¨æ§æŠ•å½±)
â”‚   â”œâ”€â”€ up_proj (ä¸ŠæŠ•å½±)
â”‚   â”œâ”€â”€ down_proj (ä¸‹æŠ•å½±)
â”‚   â””â”€â”€ act_fn (SiLUæ¿€æ´»)
â””â”€â”€ dropout
```

**æ®‹å·®è¿æ¥**ï¼šé‡‡ç”¨Pre-LNæ¶æ„
- è‡ªæ³¨æ„åŠ›ï¼š`hidden = residual + dropout(attn(norm(hidden)))`
- MLPï¼š`hidden = residual + dropout(mlp(norm(hidden)))`

### 2.3 æ³¨æ„åŠ›æœºåˆ¶ï¼šEmu3Attention

**å¤šå¤´æ³¨æ„åŠ›é…ç½®**ï¼š
- num_attention_heads: 32
- num_key_value_heads: 8 (åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›GQA)
- head_dim: 128
- num_key_value_groups: 4

**å…³é”®ç‰¹æ€§**ï¼š
1. **åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›(GQA)**ï¼š32ä¸ªæŸ¥è¯¢å¤´å…±äº«8ä¸ªé”®å€¼å¤´ï¼Œå‡å°‘å†…å­˜å ç”¨
2. **æ—‹è½¬ä½ç½®ç¼–ç (RoPE)**ï¼š
   - åŸºç¡€RoPE
   - çº¿æ€§ç¼©æ”¾RoPEï¼ˆç”¨äºé•¿åºåˆ—ï¼‰
   - åŠ¨æ€NTKç¼©æ”¾RoPEï¼ˆè‡ªé€‚åº”é•¿åºåˆ—ï¼‰
3. **æŸ¥è¯¢/é”®å½’ä¸€åŒ–**ï¼šåœ¨æŠ•å½±åå¯¹æŸ¥è¯¢å’Œé”®è¿›è¡ŒRMSNorm
4. **å¤šç§æ³¨æ„åŠ›å®ç°**ï¼š
   - eagerï¼šæ ‡å‡†å®ç°
   - flash_attention_2ï¼šé«˜æ•ˆGPUå®ç°
   - sdpaï¼šPyTorchç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›

**è®¡ç®—æµç¨‹**ï¼š
```
Q = q_norm(q_proj(hidden))  # [batch, seq_len, num_heads, head_dim]
K = k_norm(k_proj(hidden))  # [batch, seq_len, num_kv_heads, head_dim]
V = v_proj(hidden)          # [batch, seq_len, num_kv_heads, head_dim]

Q, K = apply_rotary_pos_emb(Q, K, cos, sin, position_ids)
K, V = repeat_kv(K, V, num_key_value_groups)  # æ‰©å±•åˆ°32ä¸ªå¤´

attn_weights = softmax(Q @ K^T / sqrt(head_dim))
output = attn_weights @ V
output = o_proj(output)
```

### 2.4 MLPå‰é¦ˆç½‘ç»œ

é‡‡ç”¨**é—¨æ§çº¿æ€§å•å…ƒ(GLU)**æ¶æ„ï¼š

```
output = down_proj(act_fn(gate_proj(x)) * up_proj(x))
```

- hidden_size: 4,096
- intermediate_size: 14,336 (3.5å€æ‰©å±•)
- æ¿€æ´»å‡½æ•°ï¼šSiLU (Swish)

---

## 3. è§†è§‰ç¼–ç å™¨ï¼šVision Tokenizer (IBQ)

### 3.1 IBQæ¶æ„

```
IBQ (Index-Based Quantization)
â”œâ”€â”€ Encoder (å·ç§¯ç¼–ç å™¨)
â”‚   â””â”€â”€ å°†å›¾åƒå‹ç¼©åˆ°æ½œåœ¨ç©ºé—´
â”œâ”€â”€ quant_conv (é‡åŒ–å‰æŠ•å½±)
â”œâ”€â”€ IndexPropagationQuantize (å‘é‡é‡åŒ–)
â”‚   â””â”€â”€ ç¦»æ•£ç æœ¬é‡åŒ–
â”œâ”€â”€ post_quant_conv (é‡åŒ–åæŠ•å½±)
â””â”€â”€ Decoder (å·ç§¯è§£ç å™¨)
    â””â”€â”€ ä»æ½œåœ¨ç©ºé—´é‡å»ºå›¾åƒ
```

**å…³é”®ç‰¹æ€§**ï¼š
- è¾“å…¥ï¼šRGBå›¾åƒ (HÃ—WÃ—3)
- è¾“å‡ºï¼šç¦»æ•£tokenåºåˆ— (H/16 Ã— W/16)
- å‹ç¼©æ¯”ï¼š16å€ç©ºé—´å‹ç¼©
- é‡åŒ–æ–¹æ³•ï¼šç´¢å¼•ä¼ æ’­é‡åŒ–(IPQ)

### 3.2 å›¾åƒå¤„ç†æµç¨‹

```
åŸå§‹å›¾åƒ (ä»»æ„åˆ†è¾¨ç‡)
    â†“
smart_resize (ä¿æŒå®½é«˜æ¯”ï¼Œç›®æ ‡é¢ç§¯720Ã—720)
    â†“
å½’ä¸€åŒ– ([-1, 1])
    â†“
IBQç¼–ç å™¨
    â†“
ç¦»æ•£token (H/16 Ã— W/16)
    â†“
æ ¼å¼åŒ–ä¸ºæ–‡æœ¬tokenåºåˆ—
    â†“
ä¸æ–‡æœ¬tokenäº¤é”™
```

**tokenæ ¼å¼**ï¼š
```
<|image start|>H*W<|image token|>
<|visual token 000001|><|visual token 000002|>...<|extra_200|>  # EOL
<|visual token 000017|><|visual token 000018|>...<|image end|>
```

---

## 4. æ–‡æœ¬tokenizerä¸ç‰¹æ®Štoken

### 4.1 ç‰¹æ®Štokenå®šä¹‰

| Token | ID | ç”¨é€” |
|-------|-----|------|
| BOS | 151849 | åºåˆ—å¼€å§‹ |
| EOS | 151850 | åºåˆ—ç»“æŸ |
| PAD | 151643 | å¡«å…… |
| IMG | 151851 | å›¾åƒtokenæ ‡è®° |
| BOI | 151852 | å›¾åƒå¼€å§‹ |
| EOI | 151853 | å›¾åƒç»“æŸ |
| EOL | 151846 | è¡Œç»“æŸ |
| EOF | 151847 | æ–‡ä»¶ç»“æŸ |
| BSS | 151854 | ç”Ÿæˆå¼€å§‹ |
| ESS | 151855 | ç”Ÿæˆç»“æŸ |
| BOG | 151860 | å…¨å±€CoTå¼€å§‹ |
| EOG | 151861 | å…¨å±€CoTç»“æŸ |
| BOC | 151850 | æ­¥éª¤CoTå¼€å§‹ |
| EOC | 151851 | æ­¥éª¤CoTç»“æŸ |

### 4.2 è¯æ±‡è¡¨

- æ€»å¤§å°ï¼š184,622
- æ–‡æœ¬tokenï¼š~170,000
- è§†è§‰tokenï¼š~14,000+
- ç‰¹æ®Štokenï¼š~600

---

## 5. ç”Ÿæˆè¿‡ç¨‹ä¸æ¨ç†

### 5.1 ç”Ÿæˆé…ç½®

```python
sampling_params = {
    # æ–‡æœ¬tokené‡‡æ ·
    'text_top_k': 1024,
    'text_top_p': 0.9,
    'text_temperature': 1.0,
    
    # å›¾åƒtokené‡‡æ ·
    'image_top_k': 5120,
    'image_top_p': 1.0,
    'image_temperature': 1.0,
    
    # é€šç”¨é…ç½®
    'max_new_tokens': 5120,
    'classifier_free_guidance': 5.0,  # T2Iæ¨èå€¼
    'use_cache': True,
    'use_differential_sampling': True,
}
```

### 5.2 åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼(CFG)

**å®ç°**ï¼šUnbatchedClassifierFreeGuidanceLogitsForVisualTokenProcessor

```
logits_guided = logits_cond + guidance_scale * (logits_cond - logits_uncond)
```

**ä¸‰ç§æ— æ¡ä»¶ç±»å‹**ï¼š
1. no_textï¼šæ— æ–‡æœ¬æç¤º
2. no_prev_textï¼šæ— å‰æ–‡æœ¬
3. no_prev_modalï¼šæ— å‰æ¨¡æ€

### 5.3 æ¨ç†æµç¨‹

```
è¾“å…¥æç¤º
    â†“
Tokenize (æ–‡æœ¬+å›¾åƒtoken)
    â†“
æ·»åŠ BOS token
    â†“
æ¨¡å‹å‰å‘ä¼ æ’­ (with KVç¼“å­˜)
    â†“
Logitså¤„ç† (CFG + é‡‡æ ·)
    â†“
ç”Ÿæˆtokenåºåˆ—
    â†“
è§£ç  (æ–‡æœ¬+å›¾åƒ)
    â†“
IBQè§£ç å™¨é‡å»ºå›¾åƒ
    â†“
è¾“å‡ºç»“æœ
```

---

## 6. ä»»åŠ¡ç±»å‹ä¸æ¨¡æ¿

### 6.1 æ”¯æŒçš„ä»»åŠ¡

| ä»»åŠ¡ | ç±»å‹ | æè¿° |
|------|------|------|
| T2I | text-to-image | æ–‡æœ¬ç”Ÿæˆå›¾åƒ |
| X2I | any-to-image | ä»»æ„æ¨¡æ€ç”Ÿæˆå›¾åƒ |
| Howto | æ•™ç¨‹ç”Ÿæˆ | ç”Ÿæˆæ­¥éª¤æ•™ç¨‹ |
| Story | æ•…äº‹ç”Ÿæˆ | ç”Ÿæˆäº¤é”™çš„å›¾æ–‡æ•…äº‹ |
| Explore | ä¸–ç•Œæ¢ç´¢ | ç”Ÿæˆäº¤é”™çš„æ¢ç´¢åºåˆ— |
| VLA | è§†è§‰è¯­è¨€åŠ¨ä½œ | å…·èº«AIä»»åŠ¡ |

### 6.2 æç¤ºæ¨¡æ¿

```python
# T2Iä»»åŠ¡
template = "<|extra_203|>You are a helpful assistant for t2i task. USER: {question} ASSISTANT: <|extra_100|>"
unc_prompt = "<|extra_203|>You are a helpful assistant. USER:  ASSISTANT: <|extra_100|>"

# X2Iä»»åŠ¡ï¼ˆå¸¦å‚è€ƒå›¾åƒï¼‰
template = "<|extra_203|>You are a helpful assistant for x2i task. USER: {question}<|IMAGE|> ASSISTANT: <|extra_100|>"
```

---

## 7. å…³é”®åˆ›æ–°ç‚¹

### 7.1 åŸç”Ÿå¤šæ¨¡æ€è®¾è®¡

- **æ— é€‚é…å™¨**ï¼šç›´æ¥åœ¨ç»Ÿä¸€tokenç©ºé—´ä¸­å¤„ç†è§†è§‰å’Œè¯­è¨€
- **äº¤é”™åºåˆ—**ï¼šæ”¯æŒä»»æ„é¡ºåºçš„å›¾åƒå’Œæ–‡æœ¬token
- **ç«¯åˆ°ç«¯è®­ç»ƒ**ï¼šç»Ÿä¸€çš„next-tokené¢„æµ‹ç›®æ ‡

### 7.2 é«˜æ•ˆæ¨ç†

- **KVç¼“å­˜**ï¼šåŠ é€Ÿè‡ªå›å½’ç”Ÿæˆ
- **åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›**ï¼šå‡å°‘å†…å­˜å ç”¨
- **Flash Attention 2**ï¼šGPUä¼˜åŒ–å®ç°
- **DiDAåŠ é€Ÿ**ï¼šç¦»æ•£æ‰©æ•£é€‚é…å®ç°20å€åŠ é€Ÿ

### 7.3 å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›

- **é•¿åºåˆ—ç”Ÿæˆ**ï¼šæ”¯æŒ9,216ä¸ªtokenä½ç½®
- **é«˜è´¨é‡å›¾åƒ**ï¼š720Ã—720åˆ†è¾¨ç‡
- **å¤šæ ·åŒ–è¾“å‡º**ï¼šæ”¯æŒå¤šç§å®½é«˜æ¯”å’Œç”Ÿæˆæ¨¡å¼
- **é“¾å¼æ€è€ƒ**ï¼šæ”¯æŒCoTæ¨ç†è¿‡ç¨‹å¯è§†åŒ–

---

## 8. æ¨¡å‹é…ç½®å‚æ•°æ€»ç»“

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| éšè—ç»´åº¦ | 4,096 | ä¸»è¦ç‰¹å¾ç»´åº¦ |
| å±‚æ•° | 32 | Transformerå±‚æ•° |
| æ³¨æ„åŠ›å¤´æ•° | 32 | å¤šå¤´æ³¨æ„åŠ› |
| KVå¤´æ•° | 8 | åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› |
| å¤´ç»´åº¦ | 128 | æ¯ä¸ªå¤´çš„ç»´åº¦ |
| ä¸­é—´ç»´åº¦ | 14,336 | MLPæ‰©å±•ç»´åº¦ |
| æœ€å¤§ä½ç½® | 9,216 | æœ€å¤§åºåˆ—é•¿åº¦ |
| è¯æ±‡è¡¨å¤§å° | 184,622 | tokenæ€»æ•° |
| å›¾åƒåˆ†è¾¨ç‡ | 720Ã—720 | ç”Ÿæˆå›¾åƒå¤§å° |
| å›¾åƒå‹ç¼© | 16Ã— | ç©ºé—´å‹ç¼©æ¯” |

---

## 9. æ¨ç†æ¡†æ¶æ”¯æŒ

### 9.1 Transformersåç«¯
- æ ‡å‡†PyTorchå®ç°
- æ”¯æŒFlash Attention 2
- æ”¯æŒSDPAä¼˜åŒ–

### 9.2 vLLMåç«¯
- æ¡ä»¶/æ— æ¡ä»¶æ‰¹å¤„ç†è°ƒåº¦å™¨
- 4-5å€ç«¯åˆ°ç«¯åŠ é€Ÿ
- æ”¯æŒå¼ é‡å¹¶è¡Œ

---

## 10. è¾“å‡ºæ ¼å¼

### 10.1 Protobufæ ¼å¼

ç”Ÿæˆç»“æœä¿å­˜ä¸º`.pb`æ–‡ä»¶ï¼ŒåŒ…å«ï¼š
- é—®é¢˜/æç¤º
- å‚è€ƒå›¾åƒ
- ç”Ÿæˆçš„æ–‡æœ¬æ®µ
- ç”Ÿæˆçš„å›¾åƒ
- é“¾å¼æ€è€ƒ(CoT)æ³¨é‡Š

### 10.2 å¯è§†åŒ–

```
results/<pb_name>/
â”œâ”€â”€ 000_question.txt
â”œâ”€â”€ 000_global_cot.txt
â”œâ”€â”€ 001_text.txt
â”œâ”€â”€ 001_00_image.png
â”œâ”€â”€ 001_00_image_cot.txt
â”œâ”€â”€ 002_text.txt
â”œâ”€â”€ 002_00_image.png
â””â”€â”€ video.mp4 (å¯é€‰)
```

---

## 11. è¯¦ç»†çš„å‰å‘ä¼ æ’­æµç¨‹

### 11.1 è¾“å…¥å¤„ç†é˜¶æ®µ

```
åŸå§‹è¾“å…¥
â”œâ”€â”€ æ–‡æœ¬æç¤º (str)
â”œâ”€â”€ å‚è€ƒå›¾åƒ (PIL.Image, å¯é€‰)
â””â”€â”€ ä»»åŠ¡ç±»å‹ (t2i/x2i/howto/story/explore/vla)
    â†“
æ–‡æœ¬Tokenization
â”œâ”€â”€ ä½¿ç”¨AutoTokenizer
â”œâ”€â”€ æ·»åŠ ç‰¹æ®Štoken (BOS, ä»»åŠ¡æ ‡è®°)
â””â”€â”€ è¿”å›input_ids [batch_size, seq_len]
    â†“
å›¾åƒå¤„ç† (å¦‚æœæä¾›)
â”œâ”€â”€ smart_resize (ä¿æŒå®½é«˜æ¯”)
â”œâ”€â”€ å½’ä¸€åŒ–åˆ°[-1, 1]
â”œâ”€â”€ IBQç¼–ç 
â””â”€â”€ ç”Ÿæˆè§†è§‰tokenåºåˆ—
    â†“
åˆå¹¶tokenåºåˆ—
â”œâ”€â”€ äº¤é”™æ”¾ç½®æ–‡æœ¬å’Œè§†è§‰token
â”œâ”€â”€ æ·»åŠ ç‰¹æ®Šåˆ†éš”ç¬¦ (EOL, EOFç­‰)
â””â”€â”€ æœ€ç»ˆinput_ids [batch_size, total_seq_len]
```

### 11.2 Embeddingå±‚

```
input_ids [batch_size, seq_len]
    â†“
embed_tokens (nn.Embedding)
    â†“
embeddings [batch_size, seq_len, hidden_size=4096]
    â†“
dropout (p=0.1)
    â†“
hidden_states [batch_size, seq_len, 4096]
```

### 11.3 Transformerå±‚å †æ ˆ

```
å¯¹äºæ¯ä¸€å±‚ (32å±‚):
    â†“
input_layernorm (RMSNorm)
    â†“
è‡ªæ³¨æ„åŠ› (Emu3Attention)
â”œâ”€â”€ Q = q_norm(q_proj(x))
â”œâ”€â”€ K = k_norm(k_proj(x))
â”œâ”€â”€ V = v_proj(x)
â”œâ”€â”€ åº”ç”¨RoPEä½ç½®ç¼–ç 
â”œâ”€â”€ è®¡ç®—æ³¨æ„åŠ›æƒé‡
â”œâ”€â”€ åº”ç”¨CFG (å¦‚æœå¯ç”¨)
â””â”€â”€ è¾“å‡º [batch_size, seq_len, 4096]
    â†“
æ®‹å·®è¿æ¥ + dropout
    â†“
post_attention_layernorm (RMSNorm)
    â†“
MLP (å‰é¦ˆç½‘ç»œ)
â”œâ”€â”€ gate = gate_proj(x)
â”œâ”€â”€ up = up_proj(x)
â”œâ”€â”€ output = down_proj(SiLU(gate) * up)
â””â”€â”€ è¾“å‡º [batch_size, seq_len, 4096]
    â†“
æ®‹å·®è¿æ¥ + dropout
    â†“
hidden_states [batch_size, seq_len, 4096]
```

### 11.4 è¾“å‡ºå±‚

```
hidden_states [batch_size, seq_len, 4096]
    â†“
norm (RMSNorm)
    â†“
lm_head (Linear)
    â†“
logits [batch_size, seq_len, vocab_size=184622]
    â†“
é‡‡æ ·/è´ªå¿ƒè§£ç 
    â†“
next_token_id
```

---

## 12. æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£

### 12.1 åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›(GQA)è®¡ç®—

```
æ ‡å‡†å¤šå¤´æ³¨æ„åŠ› (MHA):
Q: [batch, 32, seq_len, 128]
K: [batch, 32, seq_len, 128]
V: [batch, 32, seq_len, 128]

åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› (GQA):
Q: [batch, 32, seq_len, 128]  # 32ä¸ªæŸ¥è¯¢å¤´
K: [batch, 8, seq_len, 128]   # 8ä¸ªé”®å€¼å¤´
V: [batch, 8, seq_len, 128]   # 8ä¸ªé”®å€¼å¤´

repeat_kvæ“ä½œ:
K_expanded: [batch, 32, seq_len, 128]  # æ¯ä¸ªKVå¤´é‡å¤4æ¬¡
V_expanded: [batch, 32, seq_len, 128]  # æ¯ä¸ªKVå¤´é‡å¤4æ¬¡

æ³¨æ„åŠ›è®¡ç®—:
attn_weights = softmax(Q @ K_expanded^T / sqrt(128))  # [batch, 32, seq_len, seq_len]
output = attn_weights @ V_expanded  # [batch, 32, seq_len, 128]
```

**ä¼˜åŠ¿**ï¼š
- å†…å­˜å ç”¨å‡å°‘75% (32â†’8ä¸ªKVå¤´)
- è®¡ç®—é‡å‡å°‘75%
- æ€§èƒ½æŸå¤±æœ€å°

### 12.2 æ—‹è½¬ä½ç½®ç¼–ç (RoPE)

```
åŸºç¡€RoPE:
Î¸_i = base^(-2i/d), base=10000
å¯¹äºä½ç½®må’Œç»´åº¦i:
cos(m*Î¸_i), sin(m*Î¸_i)

åº”ç”¨åˆ°Qå’ŒK:
Q' = Q * cos(m*Î¸) + rotate_half(Q) * sin(m*Î¸)
K' = K * cos(m*Î¸) + rotate_half(K) * sin(m*Î¸)

å…¶ä¸­rotate_half(x) = [-x[d/2:], x[:d/2]]

é•¿åºåˆ—æ‰©å±•:
- çº¿æ€§ç¼©æ”¾: Î¸_i' = Î¸_i / scaling_factor
- åŠ¨æ€NTK: base' = base * (scaling_factor * L / L_max)^(d/(d-2))
```

---

## 13. ç”Ÿæˆç­–ç•¥è¯¦è§£

### 13.1 é‡‡æ ·æ–¹æ³•

```
Top-Ké‡‡æ · (æ–‡æœ¬):
1. è·å–logits
2. é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„K=1024ä¸ªtoken
3. å°†å…¶ä»–tokenè®¾ä¸º-inf
4. åº”ç”¨æ¸©åº¦ç¼©æ”¾
5. Softmax + é‡‡æ ·

Top-Pé‡‡æ · (å›¾åƒ):
1. è·å–logits
2. æŒ‰æ¦‚ç‡æ’åº
3. ç´¯ç§¯æ¦‚ç‡ç›´åˆ°è¾¾åˆ°P=1.0
4. ä¿ç•™è¿™äº›token
5. åº”ç”¨æ¸©åº¦ç¼©æ”¾
6. Softmax + é‡‡æ ·
```

### 13.2 å·®åˆ†é‡‡æ ·

```
å·®åˆ†é‡‡æ · (Differential Sampling):
- å¯¹æ–‡æœ¬å’Œå›¾åƒtokenä½¿ç”¨ä¸åŒçš„é‡‡æ ·å‚æ•°
- æ–‡æœ¬: top_k=1024, top_p=0.9, temp=1.0
- å›¾åƒ: top_k=5120, top_p=1.0, temp=1.0
- æ ¹æ®tokenç±»å‹åŠ¨æ€åˆ‡æ¢é‡‡æ ·ç­–ç•¥
```

### 13.3 åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼æµç¨‹

```
å¯¹äºæ¯ä¸ªç”Ÿæˆæ­¥éª¤:
    â†“
1. å‰å‘ä¼ æ’­(æ¡ä»¶è¾“å…¥)
   logits_cond = model(input_ids_cond)
    â†“
2. å‰å‘ä¼ æ’­(æ— æ¡ä»¶è¾“å…¥)
   logits_uncond = model(input_ids_uncond)
    â†“
3. è®¡ç®—å¼•å¯¼logits
   logits_guided = logits_cond + guidance_scale * (logits_cond - logits_uncond)
    â†“
4. é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
   next_token = sample(logits_guided)
    â†“
5. æ›´æ–°è¾“å…¥åºåˆ—
   input_ids = [input_ids, next_token]
    â†“
6. æ›´æ–°KVç¼“å­˜
   past_key_values = update_cache(past_key_values, next_token)
```

---

## 14. å†…å­˜ä¼˜åŒ–æŠ€æœ¯

### 14.1 KVç¼“å­˜

```
æ ‡å‡†è‡ªå›å½’ç”Ÿæˆ:
ç¬¬1æ­¥: è®¡ç®—æ‰€æœ‰tokençš„Q,K,V â†’ å­˜å‚¨K,V
ç¬¬2æ­¥: åªè®¡ç®—æ–°tokençš„Q,K,V â†’ é‡ç”¨æ—§K,V
...
ç¬¬Næ­¥: åªè®¡ç®—æ–°tokençš„Q,K,V â†’ é‡ç”¨æ‰€æœ‰æ—§K,V

å†…å­˜èŠ‚çœ:
- ä¸ä½¿ç”¨ç¼“å­˜: O(N^2) å†…å­˜
- ä½¿ç”¨ç¼“å­˜: O(N) å†…å­˜
- å¯¹äºN=5120: èŠ‚çœ99.96%å†…å­˜
```

### 14.2 æ¢¯åº¦æ£€æŸ¥ç‚¹

```
è®­ç»ƒæ—¶å¯ç”¨:
- ä¸ä¿å­˜ä¸­é—´æ¿€æ´»å€¼
- å‰å‘ä¼ æ’­æ—¶è®¡ç®—
- åå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—
- å†…å­˜èŠ‚çœ: ~50%
- é€Ÿåº¦æŸå¤±: ~20-30%
```

### 14.3 Flash Attention 2

```
æ ‡å‡†æ³¨æ„åŠ›:
Q @ K^T â†’ softmax â†’ dropout â†’ @ V
å†…å­˜: O(N^2) (éœ€è¦å­˜å‚¨å®Œæ•´æ³¨æ„åŠ›çŸ©é˜µ)

Flash Attention 2:
- åˆ†å—è®¡ç®—
- å‡å°‘HBMè®¿é—®
- å†…å­˜: O(N)
- é€Ÿåº¦: 2-4å€åŠ é€Ÿ
```

---

## 15. æ¨¡å‹å˜ä½“

### 15.1 Emu3.5 vs Emu3.5-Image

| ç‰¹æ€§ | Emu3.5 | Emu3.5-Image |
|------|--------|--------------|
| ç”¨é€” | é€šç”¨å¤šæ¨¡æ€ | T2I/X2Iä¸“ç”¨ |
| è®­ç»ƒæ•°æ® | 10T+äº¤é”™token | ä¼˜åŒ–çš„å›¾åƒç”Ÿæˆæ•°æ® |
| æ€§èƒ½ | å¹³è¡¡ | å›¾åƒç”Ÿæˆæœ€ä¼˜ |
| æ¨èä»»åŠ¡ | äº¤é”™ç”Ÿæˆ | å•å›¾åƒç”Ÿæˆ |

### 15.2 DiDAåŠ é€Ÿç‰ˆæœ¬

```
æ ‡å‡†NTP (Next Token Prediction):
- é¡ºåºç”Ÿæˆtoken
- æ¯æ­¥ä¸€ä¸ªtoken
- ç”Ÿæˆæ—¶é—´: O(N)

DiDA (Discrete Diffusion Adaptation):
- åŒå‘å¹¶è¡Œé¢„æµ‹
- å¤šæ­¥åŒæ—¶ç”Ÿæˆ
- ç”Ÿæˆæ—¶é—´: O(log N)
- åŠ é€Ÿ: ~20å€
```

---

## æ€»ç»“

Emu3.5é€šè¿‡**ç»Ÿä¸€çš„å¤šæ¨¡æ€tokené¢„æµ‹**æ¡†æ¶ï¼Œå®ç°äº†çœŸæ­£çš„åŸç”Ÿå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚å…¶æ ¸å¿ƒä¼˜åŠ¿åŒ…æ‹¬ï¼š

1. **æ¶æ„ç»Ÿä¸€æ€§**ï¼šæ— éœ€ç‰¹æ®Šé€‚é…å™¨ï¼Œè§†è§‰å’Œè¯­è¨€å…±äº«åŒä¸€Transformer
2. **è®­ç»ƒæ•ˆç‡**ï¼š10T+tokençš„å¤§è§„æ¨¡é¢„è®­ç»ƒ
3. **æ¨ç†æ•ˆç‡**ï¼šKVç¼“å­˜ã€GQAã€Flash Attentionç­‰å¤šé‡ä¼˜åŒ–
4. **ç”Ÿæˆè´¨é‡**ï¼šæ”¯æŒé«˜åˆ†è¾¨ç‡ã€å¤šæ ·åŒ–çš„å›¾åƒç”Ÿæˆ
5. **ä»»åŠ¡å¤šæ ·æ€§**ï¼šæ”¯æŒT2Iã€X2Iã€äº¤é”™ç”Ÿæˆç­‰å¤šç§ä»»åŠ¡

è¿™ä½¿Emu3.5æˆä¸ºä¸€ä¸ªçœŸæ­£çš„**ä¸–ç•Œå­¦ä¹ è€…**ï¼Œèƒ½å¤Ÿåœ¨ç»Ÿä¸€çš„tokenç©ºé—´ä¸­ç†è§£å’Œç”Ÿæˆå¤æ‚çš„å¤šæ¨¡æ€å†…å®¹ã€‚

---

## 16. ä»£ç å®ç°å…³é”®ç‚¹

### 16.1 æ¨¡å‹åˆå§‹åŒ–

```python
# ä»é…ç½®åŠ è½½
model_config = Emu3Config.from_pretrained(model_path)
model = Emu3ForCausalLM.from_pretrained(
    model_path,
    config=model_config,
    torch_dtype=torch.bfloat16,  # ä½¿ç”¨BF16æ··åˆç²¾åº¦
    device_map="auto",            # è‡ªåŠ¨è®¾å¤‡æ˜ å°„
    attn_implementation="flash_attention_2"  # ä½¿ç”¨Flash Attention
)

# åˆå§‹åŒ–è§†è§‰ç»„ä»¶
tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
vq_model = build_vision_tokenizer("ibq", vq_path)
```

### 16.2 å‰å‘ä¼ æ’­å…³é”®ä»£ç 

```python
# Emu3Model.forward()
def forward(self, input_ids, attention_mask=None, position_ids=None, ...):
    # 1. åµŒå…¥å±‚
    hidden_states = self.embed_tokens(input_ids)
    hidden_states = self.dropout(hidden_states)

    # 2. å‡†å¤‡æ³¨æ„åŠ›æ©ç 
    if self._use_flash_attention_2:
        attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None
    else:
        attention_mask = _prepare_4d_causal_attention_mask(...)

    # 3. é€šè¿‡æ‰€æœ‰å±‚
    for decoder_layer in self.layers:
        if self.gradient_checkpointing and self.training:
            layer_outputs = self._gradient_checkpointing_func(
                decoder_layer.__call__,
                hidden_states,
                attention_mask,
                position_ids,
                past_key_values,
                ...
            )
        else:
            layer_outputs = decoder_layer(
                hidden_states,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_value=past_key_values,
                ...
            )
        hidden_states = layer_outputs[0]

    # 4. æœ€ç»ˆå½’ä¸€åŒ–
    hidden_states = self.norm(hidden_states)

    return BaseModelOutputWithPast(
        last_hidden_state=hidden_states,
        past_key_values=next_cache,
        ...
    )
```

### 16.3 æ³¨æ„åŠ›è®¡ç®—å…³é”®ä»£ç 

```python
# Emu3Attention.forward()
def forward(self, hidden_states, attention_mask=None, position_ids=None, ...):
    bsz, q_len, _ = hidden_states.size()

    # æŠ•å½±Q,K,V
    query_states = self.q_proj(hidden_states)
    key_states = self.k_proj(hidden_states)
    value_states = self.v_proj(hidden_states)

    # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
    query_states = self.q_norm(query_states.view(bsz, q_len, self.num_heads, self.head_dim)).transpose(1, 2)
    key_states = self.k_norm(key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim)).transpose(1, 2)
    value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

    # åº”ç”¨RoPE
    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)

    # æ›´æ–°KVç¼“å­˜
    if past_key_value is not None:
        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, ...)

    # æ‰©å±•KVåˆ°å®Œæ•´å¤´æ•°
    key_states = repeat_kv(key_states, self.num_key_value_groups)
    value_states = repeat_kv(value_states, self.num_key_value_groups)

    # è®¡ç®—æ³¨æ„åŠ›
    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)

    if attention_mask is not None:
        attn_weights = attn_weights + attention_mask

    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)

    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()
    attn_output = attn_output.reshape(bsz, q_len, -1)
    attn_output = self.o_proj(attn_output)

    return attn_output, attn_weights, past_key_value
```

### 16.4 ç”Ÿæˆè¿‡ç¨‹å…³é”®ä»£ç 

```python
# ç”Ÿæˆå‡½æ•°
@torch.no_grad()
def generate(cfg, model, tokenizer, input_ids, unconditional_ids, ...):
    # æ„å»ºlogitså¤„ç†å™¨
    logits_processor = LogitsProcessorList([
        UnbatchedClassifierFreeGuidanceLogitsForVisualTokenProcessor(
            guidance_scale=cfg.classifier_free_guidance,
            model=model,
            tokenizer=tokenizer,
            unconditional_ids=unconditional_ids,
            full_unconditional_ids=full_unconditional_ids,
            force_same_image_size=force_same_image_size,
        )
    ])

    # ç”Ÿæˆé…ç½®
    generation_config = GenerationConfig(
        **cfg.sampling_params,
        pad_token_id=cfg.special_token_ids["PAD"],
        eos_token_id=cfg.special_token_ids["EOS"],
    )

    # è°ƒç”¨æ¨¡å‹ç”Ÿæˆ
    outputs = model.generate(
        input_ids,
        generation_config=generation_config,
        logits_processor=logits_processor,
    )

    return outputs
```

### 16.5 å›¾åƒç¼–ç å…³é”®ä»£ç 

```python
# å›¾åƒå¤„ç†æµç¨‹
@torch.no_grad()
def build_image(image, cfg, tokenizer, vq_model):
    # 1. è°ƒæ•´å¤§å°
    image = smart_resize(image, cfg.image_area)
    w, h = image.size

    # 2. è·å–è®¾å¤‡å’Œæ•°æ®ç±»å‹
    device = next(vq_model.parameters()).device
    dtype = next(vq_model.parameters()).dtype

    # 3. å½’ä¸€åŒ–åˆ°[-1, 1]
    image = torch.tensor((np.array(image) / 127.5 - 1.0)).to(device, dtype).permute(2, 0, 1)

    # 4. IBQç¼–ç 
    _, _, token = vq_model.encode(image[None])

    # 5. é‡å¡‘token (H/16 Ã— W/16)
    token = token[-1].view(h // 16, w // 16)

    # 6. æ ¼å¼åŒ–ä¸ºæ–‡æœ¬tokenåºåˆ—
    return format_image_string(tokenizer, token)

# Tokenæ ¼å¼åŒ–
def format_image_string(tokenizer, image_tokens):
    image_string = ""
    h, w = image_tokens.shape
    for _h in range(h):
        row_string = ""
        for _w in range(w):
            row_string += "<|visual token {token_id:0>6d}|>".format(token_id=image_tokens[_h, _w])
        if _h < h - 1:
            row_string += tokenizer.eol_token
        image_string += row_string

    return "{image_start}{token_height}*{token_width}{image_token}{token_str}{image_end}".format(
        image_start=tokenizer.boi_token,
        token_height=h,
        token_width=w,
        image_token=tokenizer.img_token,
        token_str=image_string,
        image_end=tokenizer.eoi_token,
    )
```

---

## 17. æ€§èƒ½æŒ‡æ ‡

### 17.1 æ¨¡å‹å¤§å°

| ç»„ä»¶ | å‚æ•°é‡ | è¯´æ˜ |
|------|--------|------|
| åµŒå…¥å±‚ | ~755M | vocab_size Ã— hidden_size |
| 32å±‚Transformer | ~6.7B | ä¸»è¦å‚æ•° |
| LM Head | ~755M | hidden_size Ã— vocab_size |
| **æ€»è®¡** | **~8.2B** | çº¦82äº¿å‚æ•° |

### 17.2 æ¨ç†æ€§èƒ½

| æŒ‡æ ‡ | å€¼ | è¯´æ˜ |
|------|-----|------|
| å•GPUå†…å­˜ | ~16GB | BF16ç²¾åº¦ |
| åŒGPUå†…å­˜ | ~32GB | å¼ é‡å¹¶è¡Œ |
| T2Iç”Ÿæˆæ—¶é—´ | 2-5åˆ†é’Ÿ | æ ‡å‡†NTP |
| T2Iç”Ÿæˆæ—¶é—´(DiDA) | 6-15ç§’ | 20å€åŠ é€Ÿ |
| ååé‡ | 100-200 tokens/s | å•GPU |

### 17.3 è´¨é‡æŒ‡æ ‡

| ä»»åŠ¡ | æŒ‡æ ‡ | æ€§èƒ½ |
|------|------|------|
| T2I | FID | ä¸Gemini 2.5ç›¸å½“ |
| X2I | LPIPS | ä¼˜äºåŸºçº¿ |
| äº¤é”™ç”Ÿæˆ | ä¸€è‡´æ€§ | ä¼˜äºå•ç‹¬æ¨¡å‹ |

---

## 18. å¸¸è§é—®é¢˜ä¸ä¼˜åŒ–å»ºè®®

### 18.1 å†…å­˜ä¼˜åŒ–

```python
# 1. ä½¿ç”¨BF16æ··åˆç²¾åº¦
model = Emu3ForCausalLM.from_pretrained(..., torch_dtype=torch.bfloat16)

# 2. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹(è®­ç»ƒæ—¶)
model.gradient_checkpointing_enable()

# 3. ä½¿ç”¨Flash Attention 2
model = Emu3ForCausalLM.from_pretrained(..., attn_implementation="flash_attention_2")

# 4. å¯ç”¨KVç¼“å­˜
generation_config.use_cache = True

# 5. ä½¿ç”¨vLLMåç«¯
# 4-5å€ç«¯åˆ°ç«¯åŠ é€Ÿ
```

### 18.2 ç”Ÿæˆè´¨é‡ä¼˜åŒ–

```python
# 1. è°ƒæ•´CFGå¼ºåº¦
classifier_free_guidance = 5.0  # T2Iæ¨è
classifier_free_guidance = 2.0  # Emu3.5æ¨è

# 2. è°ƒæ•´é‡‡æ ·å‚æ•°
text_top_k = 1024      # æ–‡æœ¬å¤šæ ·æ€§
image_top_k = 5120     # å›¾åƒå¤šæ ·æ€§
temperature = 1.0      # ä¿æŒé»˜è®¤

# 3. ä½¿ç”¨å·®åˆ†é‡‡æ ·
use_differential_sampling = True

# 4. é€‰æ‹©åˆé€‚çš„æ¨¡å‹
# T2I/X2I: ä½¿ç”¨Emu3.5-Image
# äº¤é”™ç”Ÿæˆ: ä½¿ç”¨Emu3.5
```

### 18.3 é•¿åºåˆ—å¤„ç†

```python
# 1. ä½¿ç”¨RoPEç¼©æ”¾
rope_scaling = {
    "type": "dynamic",  # æˆ– "linear"
    "factor": 2.0
}

# 2. å¢åŠ æœ€å¤§ä½ç½®
max_position_embeddings = 18432  # 2å€

# 3. ä½¿ç”¨Flash Attention 2
# è‡ªåŠ¨å¤„ç†é•¿åºåˆ—
```

---

## 19. ä¸å…¶ä»–æ¨¡å‹çš„å¯¹æ¯”

### 19.1 ä¸Gemini 2.5çš„å¯¹æ¯”

| ç‰¹æ€§ | Emu3.5 | Gemini 2.5 |
|------|--------|-----------|
| å‚æ•°é‡ | 8.2B | æœªå…¬å¼€ |
| å¤šæ¨¡æ€ | åŸç”Ÿ | åŸç”Ÿ |
| T2Iè´¨é‡ | ç›¸å½“ | ç›¸å½“ |
| äº¤é”™ç”Ÿæˆ | ä¼˜ç§€ | æœªçŸ¥ |
| å¼€æº | æ˜¯ | å¦ |
| å¯æœ¬åœ°éƒ¨ç½² | æ˜¯ | å¦ |

### 19.2 ä¸LLaVAçš„å¯¹æ¯”

| ç‰¹æ€§ | Emu3.5 | LLaVA |
|------|--------|-------|
| æ¶æ„ | ç»Ÿä¸€ | é€‚é…å™¨ |
| ç”Ÿæˆèƒ½åŠ› | å›¾åƒ+æ–‡æœ¬ | ä»…æ–‡æœ¬ |
| å‚æ•°é‡ | 8.2B | 7B-13B |
| è®­ç»ƒæ•°æ® | 10T+token | 600Kå›¾åƒ |
| æ¨ç†é€Ÿåº¦ | å¿« | å¿« |

---

## 20. æ‰©å±•ä¸æ”¹è¿›æ–¹å‘

### 20.1 å¯èƒ½çš„æ”¹è¿›

1. **æ›´å¤§æ¨¡å‹**ï¼š32B/70Bå‚æ•°ç‰ˆæœ¬
2. **æ›´å¤šè¯­è¨€**ï¼šå¤šè¯­è¨€æ”¯æŒ
3. **è§†é¢‘ç†è§£**ï¼šå®Œæ•´è§†é¢‘å¤„ç†
4. **å®æ—¶äº¤äº’**ï¼šæµå¼ç”Ÿæˆä¼˜åŒ–
5. **å…·èº«AI**ï¼šæœºå™¨äººæ§åˆ¶é›†æˆ

### 20.2 åº”ç”¨åœºæ™¯

1. **å†…å®¹åˆ›ä½œ**ï¼šè‡ªåŠ¨ç”Ÿæˆå›¾æ–‡å†…å®¹
2. **æ•™è‚²**ï¼šäº¤äº’å¼æ•™å­¦å†…å®¹ç”Ÿæˆ
3. **ç”µå•†**ï¼šäº§å“æè¿°å’Œå›¾åƒç”Ÿæˆ
4. **æ¸¸æˆ**ï¼šæ¸¸æˆèµ„æºè‡ªåŠ¨ç”Ÿæˆ
5. **ç§‘ç ”**ï¼šæ•°æ®å¢å¼ºå’Œå¯è§†åŒ–
6. **å…·èº«AI**ï¼šæœºå™¨äººè§†è§‰-è¯­è¨€ç†è§£

---

## æœ€ç»ˆæ€»ç»“

Emu3.5ä»£è¡¨äº†å¤šæ¨¡æ€AIçš„ä¸€ä¸ªé‡è¦è¿›æ­¥ï¼š

âœ… **ç»Ÿä¸€æ¶æ„**ï¼šæ— éœ€å¤æ‚çš„é€‚é…å™¨å’Œç‰¹æ®Šè®¾è®¡
âœ… **é«˜æ•ˆæ¨ç†**ï¼šå¤šé‡ä¼˜åŒ–æŠ€æœ¯å®ç°å¿«é€Ÿç”Ÿæˆ
âœ… **å¼ºå¤§ç”Ÿæˆ**ï¼šæ”¯æŒé«˜è´¨é‡çš„å›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆ
âœ… **çµæ´»ä»»åŠ¡**ï¼šæ”¯æŒå¤šç§å¤šæ¨¡æ€ä»»åŠ¡
âœ… **å¼€æºå¯ç”¨**ï¼šå®Œæ•´ä»£ç å’Œæƒé‡å…¬å¼€å‘å¸ƒ

é€šè¿‡æ·±å…¥ç†è§£å…¶æ¶æ„å’Œå®ç°ç»†èŠ‚ï¼Œå¼€å‘è€…å¯ä»¥ï¼š
- é«˜æ•ˆéƒ¨ç½²å’Œä½¿ç”¨Emu3.5
- é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒ
- é›†æˆåˆ°è‡ªå·±çš„åº”ç”¨ä¸­
- ä¸ºå¤šæ¨¡æ€AIçš„å‘å±•åšå‡ºè´¡çŒ®

